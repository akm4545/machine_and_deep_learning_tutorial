# MNIST
# 머신러닝과 딥러닝을 처음 배울 때 많이 사용하는 데이터셋이 있는데
# 머신러닝은 붓꽃 데이터셋을 많이 쓰고 딥러닝은 MNIST 데이터셋이 유명하다
# MNIST는 손으로 쓴 0~9까지의 숫자로 이루어져 있다

# 텐서플로를 사용해 데이터 불러오기
# 텐서플로의 케라스 패키지를 임포트하고 패션 MNIST 데이터를 다운로드
from tensorflow import keras
(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()

# keras.datasets.fashion_mnist 모듈 아래 load_data() 함수는 훈련 데이터와 테스트 데이터를 나누어 반환
# 이 데이터는 각각 입력과 타깃의 쌍으로 구성

# 전달받은 데이터의 크기 확인
print(train_input.shape, train_target.shape)
# (60000, 28, 28) (60000,)

# 훈련 데이터는 60000개의 이미지로 이루어져 있고 각 이미지는 28 X 28 크기다
# 타깃도 60000개의 원소가 있는 1차원 배열이다

# 테스트 세트의 크기 확인
print(test_input.shape, test_target.shape)
# (10000, 28, 28) (10000,)

# 훈련 데이터에서 몇 개의 샘플 출력
import matplotlib.pyplot as plt

fig, axs = plt.subplots(1, 10, figsize=(10, 10))

for i in range(10):
    axs[i].imshow(train_input[i], cmap='gray_r')
    axs[i].axis('off')

plt.show()

# 파이썬의 리스트 내포를 사용해서 처음 10개 샘플의 타깃값을 리스트로 만든 후 출력
print([train_target[i] for i in range(10)])
# [9, 0, 0, 3, 0, 2, 7, 2, 5, 5]

# 패션 MNIST의 타깃은 0~9까지의 숫자 레이블로 구성
# 패션 MNIST에 포함된 10개 레이블의 의미는 다음과 같다
# 0      1   2      3     4    5    6   7        8   9
# 티셔츠 바지 스웨터 드레스 코트 샌달 셔츠 스니커즈 가방 앵클부츠

# 넘파이 unique() 함수로 레이블 당 샘플 개수를 확인
import numpy as np 
print(np.unique(train_target, return_counts=True))
# (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8), array([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000]))

# 이 훈련 샘플은 60000개나 되기 때문에 전체 데이터를 한꺼번에 사용하여 모델을 훈련하는 것보다 샘플을 
# 하나씩 꺼내서 모델을 훈련하는 방법이 더 효율적이다

# 이런 상황에 잘 맞는 방법이 확률적 경사 하강법이다

# SGDClassifier를 사용할 때 표준화 전처리된 데이터를 사용했다
# 확률적 경사 하강법은 여러 특성 중 기울기가 가장 가파른 방향을 따라 이동한다
# 만약 특성마다 값의 범위가 많이 다르면 올바르게 손실 함수의 경사를 내려올 수 없다

# 패션 MNIST의 경우 각 픽셀은 0~255 사이의 정숫값을 가진다
# 이런 이미지의 경우 보통 255로 나누어 0~1 사이의 값으로 정규화한다
# 이는 표준화는 아니지만 양수 값으로 이루어진 이미지를 전처리할 때 널리 사용하는 방법이다

# SGDClassifier는 2차원 입력을 다루지 못한다
# reshape() 메서드를 사용해 2차원 배열인 각 샘플을 1차원 배열로 펼친다
train_scaled = train_input / 255.0
train_scaled = train_scaled.reshape(-1, 28 * 28)

# reshape() 메서드의 두 번째 매개변수를 28 X 28 이미지 크기에 맞게 지정하면 
# 첫 번째 차원(샘플 개수)은 변하지 않고 원본 데이터의 두 번째,세 번째 차원이 1차원으로 합쳐진다

# 변환된 train_scaled의 크기 확인
print(train_scaled.shape)
# (60000, 784)

# 784개의 픽셀로 이루어진 60000개의 샘플
# SGDClassifier 클래스와 cross_validate 함수를 사용해 교차 검증으로 성능 확인
from sklearn.model_selection import cross_validate
from sklearn.linear_model import SGDClassifier

sc = SGDClassifier(loss='log', max_iter=5, random_state=42)
scores = cross_validate(sc, train_scaled, train_target, n_jobs=-1)

print(np.mean(scores['test_score']))
# 0.8196000000000001

# 로지스틱 회귀 공식을 패션 MNIST 데이터에 맞게 변형하면 
# 촐 784개의 특성이 있으므로 아주 긴 식이 만들어진다
# 가중치 개수도 많아진다

# 인공 신경망
# 가장 기본적인 인공 신경망은 확률적 경사 하강법을 사용하는 로지스틱 회귀와 같다

# 출력층(output layer)
# 클래스를 계산하고 이를 바탕으로 클래스를 예측하기 때문에 신경망의 최종 값을 
# 만든다는 의미에서 출력층이라고 부른다

# 뉴련(neuron)
# 인공 신경망에서는 z 값을 계산하는 단위를 뉴런이라고 부른다
# 뉴런에서 일어나는 일은 선형 계산이 전부다
# 이제는 뉴런이란 표현 대신 유닛(unit)이라고 부르는 사람이 더 많아지고 있다

# 입력층(input layer)
# 입력층은 특성 자체이고 특별한 계산을 수행하지 않는다
# 많은 사람이 입력층이라고 부른다

# 매컬러-피츠 뉴런
# 1943년 워런 매컬러와 월터 피츠가 제안한 뉴런 모델
# 이런 인공 뉴런은 생물학적 뉴런에서 영감을 얻어 만들어졌다
# 인공 뉴런은 생물학적 뉴런의 모양을 본뜬 수학 모델에 불과하다
# 인공 신경망은 정말 우리의 뇌에 있는 뉴런과 같지 않다
# 인공 신경망은 기존의 머신러닝 알고리즘이 잘 해결하지 못했던 문제에서 높은 성능을 발휘하는
# 새로운 종류의 머신러닝 알고리즘일 뿐이다

# 딥러닝
# 인공 신경망과 거의 동의어로 사용되는 경우가 많다
# 혹은 심층 신경망(depp neural network, DNN)을 딥러닝이라고 부른다
# 심층 신경망은 여러 개의 층을 가진 인공 신경망이다

# 확률적 경사 하강법을 사용한 로지스틱 회귀 모델이 가장 간단한 인공 신경망이라면 인공 신경망을 만들어도
# 성능이 좋아지지 않을것 같지만
# 인공 신경망 모델을 만드는 최신 라이브러리들은 SGDClassifier에는 없는 몇 가지 기능을 제공한다
# 이런 기능 덕택에 더 좋은 성능을 얻을 수 있다

# 텐서플로
# 구글이 2015년 11월 오픈소스로 공개한 딥러닝 라이브러리
# 이때를 기점으로 딥러닝에 대한 개발자의 관심이 늘어났다

# 텐서플로 임포트 
import tensorflow as tf

# 텐서플로에는 저수준 API와 고수준 API가 있다
# 케라스(Keras)가 텐서플로의 고수준 API이다
# 케라스는 2015년 3월 프랑소와 숄레가 만든 딥러닝 라이브러리이다

# 딥러닝 라이브러리가 다른 머신러닝 라이브러리와 다른 점 중 하나는 그래픽 처리
# 장치인 GPU를 사용하여 인공 신경망을 훈련한다
# GPU는 벡터와 행렬 연산에 매우 최적화되어 있기 때문에 곱셈과 덧셈이 많이 수행되는 
# 인공 신경망에 큰 도움이 된다

# 케라스 라이브러리는 직접 GPU 연산을 수행하지 않는다
# 대신 GPU 연산을 수행하는 다른 라이브러리를 백엔드로 사용한다
# 예를 들면 텐서플로가 케라스의 벡엔드 중 하나이다
# 이외에도 씨아노,CNTK와 같은 여러 딥러닝 라이브러리를 케라스 백엔드로 사용할 수 있다
# 이런 케라스를 멀티-백엔드 케라스라고 부른다
# 케라스 API만 익히면 다양한 딥러닝 라이브러리를 골라서 쓸 수 있다
# 이를 위해 케라스는 직관적이고 사용하기 편한 고수준 API를 제공한다

# 텐서플로 라이브러리에 케라스 API가 내장되어 있다
# 텐서플로 2.0 부터는 케라스 API를 남기고 나머지 고수준 API를 모두 정리했고 케라스는 
# 텐서플로의 핵심 API가 되었다

# 다양한 백엔드를 지원했던 멀티-백엔드 케라스는 2.3.1 버전 이후로 더 이상 개발되지 않는다
# 이제는 케라스와 텐서플로가 거의 동의어가 되었다

# 로지스틱 회귀에서는 교차 검증을 사용해 모델을 평가했지만
# 인공 신경망에서는 교차 검증을 잘 사용하지 않고 검증 세트를 별도로 덜어내어 사용

# 이렇게 하는 이유
# 1. 딥러닝 분야의 데이터셋은 충분히 크기 때문에 검증 점수가 안정적
# 2. 교차 검증을 수행하기에는 훈련 시간이 너무 오래 걸린다
# 어떤 딥러닝 모델은 훈련하는데 몇 시간, 심지어 며칠이 걸릴 수도 있다

# 패션 MNIST 데이터셋의 검증 세트를 나누기
from sklearn.model_selection import train_test_split

train_scaled, val_scaled, train_target, val_target = train_test_split(
    train_scaled, train_target, test_size=0.2, random_state=42)

# 훈련 세트의 20%를 검증 세트로 덜어 내었다
# 훈련 세트와 검증 세트의 크기 출력
print(train_scaled.shape, train_target.shape)
# (48000, 784) (48000,)

print(val_scaled.shape, val_target.shape)
# (12000, 784) (12000,)

# 밀집층(dense layer)
# 케라스의 레이어(keras.layers) 패키지 안에 들어있는 가장 기본이 되는 층
# 특성과 뉴런을 연결한 선이 밀집되어 있다

# 이런 층을 양쪽의 뉴런이 모두 연결하고 있기 때문에 완전 연결층(fully connected layer)이라고도부른다

# 케라스의 Dense 클래스를 사욯해 밀집층 생성
# 필요한 매개변수는 뉴런 개수, 뉴련의 출력에 적용할 함수, 입력의 크기이다
dense = keras.layers.Dense(10, activation='softmax', input_shape=(784, ))

# 뉴런 개수 10개 = 10개의 패션 아이템 분류
# activation='softmax' = 10개의 뉴런에서 출력되는 값을 확률로 바꾸기 위해서 소프트맥스 함수 사용
# 만약 2개의 클래스를 분류하는 이진 분류라면 시그모이드 함수를 사용하기 위해
# activation='sigmoid'와 같이 설정
# input_sahpe=(784,) = 입력값 크기 10개의 뉴런이 각각 몇 개의 입력을 받는지 튜플로 지정
# 여기서는 784개의 픽셀값을 받는다

# 밀집 신경망층을 가진 신경망 모델을 만들기 위해 케라스의 Sequential 클래스 사용
model = keras.Sequential(dense)

# Sequential 클래스의 객체를 만들 때 앞에서 만든 밀집층의 객체 dense를 전달했다
# 여기서 만든 model 객체가 신경망 모델이다

# 절편이 뉴런마다 더해진다

# 활성화 함수(activation function)
# 소프트맥스와 같이 뉴련의 선형 방정식 계산 결과에 적용되는 함수를 활성화 함수라고 부른다

# 시그모이드 함수나 소프트맥스와 같은 활성화 함수는 뉴런의 출력에 바로 적용되기 때문에
# 보통 층의 일부로 나타낸다
# 하지만 종종 "소프트맥스 층을 적용했어" 와 같이 따로 부르는 경우도 많다
# 가중치와 절편으로 선형 계산을 수행하는 층을 좁은 개념의 신경망 층으로 생각한다면
# 소프트맥스 층은 넓은 의미의 층이라 볼 수 있다
# 케라스 API에서도 층의 개념을 폭넓게 적용하고 있다

# 케라스 모델은 훈련하기 전에 설정 단계가기 있다
# 이런 설정을 model 객체의 compile() 메서드에서 수행
# 꼭 지정해야 할 것은 손실 함수의 종류이다
# 그 다음 훈련 과정에서 계산하고 싶은 측정값을 지정한다
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')

# 케라스의 엔트로피 손실 함수 
# 이진 분류:loss = 'binary_crossentropy' / 이진 크로스 엔트로피 손실 함수
# 다중 분류:loss = 'categorical_crossentropy' / 크로스 엔트로피 손실 함수

# 이진 크로스 엔트로피 손실을 위해 -log(예측 확률)에 타깃값(정답)을 곱했다
# 이진 분류에서는 출력층의 뉴런이 하나다
# 이 뉴런이 출력하는 확률값 a(시그모이드 함수의 출력값)를 사용해 양성 클래스와 음성 클래스에 대한 크로스 엔트로피를 계산한다

# 이진 분류의 출력 뉴런은 오직 양성 클래스에대한 확률(a)만 출력하기 때문에 음성 클래스에 대한 확률은
# 간단히 1-a로 구할 수 있다
# 이진 분류의 타깃값은 양성 샘플일 경우에는 1, 음성 샘플일 경우에는 0으로 되어있다
# 0을 곱하면 어떤 계산이든지 0이 되기 때문에 특별히 음성 샘플일 경우 1로 바꾸어 (1-타깃값) 계산한다
# 이렇게 하면 하나의 뉴런만으로 양성과 음성 클래스에 대한 크로스 엔트로피 손실을 모두 계산할 수 있다

# 패션 MNIST 데이터셋과 같이 다중 분류일 경우
# 출력층에 10개의 뉴런이 있고 10개의 클래스에 대한 확률을 출력한다
# 이진 분류와 달리 각 클래스에 대한 확률이 모두 출력되기 때문에 타깃에 
# 해당하는 확률만 남겨 놓기 위해서 나머지 확률에는 모두 0을 곱한다

# 샘플이 티셔츠일 경우 티셔츠 뉴런의 활성화 함수 출력에 크로스 엔트로피 손실 함수를 적욯하고
# 나머지 함수 출력까지는 모두 0으로 만든다
# 이렇게 하기 위해서 샘플의 타깃값은 첫 번째 원소만 1이고 나머지는 모두 0인 배열로 만들 수 있다
# 이 배열과 출력층의 활성화 값의 배열과 곱한다
# 길이가 같은 넘파이 배열의 곱셈은 원소별 곱셈으로 수행된다
# 결국 다른 원소는 모두 0이 되고 티셔츠의 티셔츠의 원소만 남는다
# 신경망은 티셔츠 샘플에서 손실을 낮추려면 티셔츠의 뉴런 활성화 출력의 값을 가능한 1에 가깝게 만들어야 한다
# 이것이 크로스 엔트로피 손실 함수가 신경망에 원하는 바이다

# 이와 같이 타깃값을 해당 클래스만 1이고 나머지는 모두 0인 배열로 만드는 것은 원-핫 인코딩(one-hot encoding)이라고 부른다
# 따라서 다중 분류에서 크로스 엔트로피 손실 함수를 사용하려면 0,1,2 와 같이 정수로 된 타깃값을
# 원-핫 인코딩으로 변환해야 한다

# 패션 MNIST 데이터의 타깃값
print(train_target[:10])
# [7 3 5 8 6 9 3 3 9 9]

# 모두 정수로 되어 있다
# 하지만 텐서플로에서는 정수로 된 타깃값을 원-핫 인코딩으로 바꾸지 않고 그냥 사용할 수 있다
# 정수로된 타깃값을 사용해 크로스 엔트로피 손실을 계산하는 것이 sparse_categorical_crossentropy이다

# 만약 타깃값을 원-핫 인코딩으로 준비했다면 compile() 메서드에 손실 함수를 loss='categorical_crossentropy로 지정한다

# metrics
# 케라스는 모델이 훈련할 때마다 기본으로 에포크마다 손실 값을 출력
# 손실이 줄어드는 것을 보고 훈련이 잘되었다는 것을 알 수 있지만 정확도도 함께 출력하기 위해
# metrics 매개변수에 정확도 지표를 의미하는 accuracy를 지정했다

# 모델 훈련의 fit() 메서드는 사이킷런과 매우 비슷하다
# 처음 두 매개변수에 입력과 타깃을 지정
# 그다음 반복할 에포크 횟수를 epochs 매개변수로 지정
model.fit(train_scaled, train_target, epochs=5)

# Epoch 1/5
# 1500/1500 [==============================] - 4s 2ms/step - loss: 0.6060 - accuracy: 0.7944
# Epoch 2/5
# 1500/1500 [==============================] - 3s 2ms/step - loss: 0.4752 - accuracy: 0.8400
# Epoch 3/5
# 1500/1500 [==============================] - 3s 2ms/step - loss: 0.4508 - accuracy: 0.8470
# Epoch 4/5
# 1500/1500 [==============================] - 3s 2ms/step - loss: 0.4376 - accuracy: 0.8525
# Epoch 5/5
# 1500/1500 [==============================] - 4s 3ms/step - loss: 0.4301 - accuracy: 0.8546

# 텐서플로와 같은 딥러닝 라이브러리는 인공 신경망을 만들고 훈련할 때 랜덤하게 동작하는 특성이 있어서
# 그때마다 결과가 조금씩 달라진다
# 모델이 최적점에 안정적으로 수렴한다면 일반적으로 이 차이는 크지 않다

# 손실(loss)과 정확도(accurancy)를 출력
# 5번 반복에 정확도가 85%를 넘었다

# 검증 세트에서 모델의 성능 확인 
# 케라스에서 모델의 성능을 평가하는 메서드는 evaluate() 메서드다
model.evaluate(val_scaled, val_target)
# 375/375 [==============================] - 1s 2ms/step - loss: 0.4498 - accuracy: 0.8484

# 검증 세트의 점수는 훈련 세트 점수보다 조금 낮은 것이 일반적이다
