심층 신경망
2개 이상의 층을 포함한 신경망
종종 다층 인공 신경망, 심층 신경망, 딥러닝을 같은 의미로 사용

렐루 함수
이미지 분류 모델의 은닉층에 많이 사용하는 활성화 함수
시그모이드 함수는 층이 많을수록 활성화 함수의 양쪽 끝에서 변화가 작기 떄문에
학습이 어려워진다
렐루 함수는 이런 문제가 없으며 계산도 간단하다

옵티마이저
신경망의 가중치와 절편을 학습하기 위한 알고리즘 또는 방법을 만한다
케라스에는 다양한 경사 하강법 알고리즘이 구현되어 있다
대표적으로 SGD, 네스테로프 모멘텀, RMSprop, Adam 등이 있다

TensorFlow
1. add() 
케라스 모델에 층을 추가하는 메서드
케라스 모델의 add() 메서드는 keras.layers 패키지 아래에 있는 층의 객체를 입력받아
신경망 모델에 추가한다. add() 메서드를 호출하여 전달한 손서대로 층이 차례대로 늘어난다

2. summary() 
케라스 모델의 정보를 출력하는 메서드
모델에 추가된 층의 종류와 순서, 모델 파라미터 개수를 출력한다
층을 만들 때 name 매개변수로 이름을 지정하면 summary() 메서드 출력에서 구분하기 쉽다

3. SGD
기본 경사 하강법 옵티마이저 클래스
learning_rate 매개변수로 학습률을 지정하며 기본값 0.01
momentum 매개변수에 0 이상의 값을 지정하면 모멘텀 최적화를 수행
nesterov 매개변수를 True로 설정하면 네스테로프 모멘텀 최적화를 수행

4. Adagrad
Adagrad 옵티마이저 클래스
learning_rate 매개변수로 학습률을 지정하며 기본값은 0.001
Adagrad는 그레이디언트 제곱을 누적하여 학습률을 나눈다
initial_accumulator_value 매개변수에서 누적 초깃값을 지정할 수 있으며 기본값은 0.1

5. RMSprop
RMSprop 옵티마이저 클래스
learning_rate 매개변수로 학습률을 지정하며 기본값 0.001
Adagrad처럼 그레이디언트 제곱으로 학습률을 나누지만 최근의 그레이디언트를 사용하기 
위해 지수 감소를 사용 
rho 개매변수에서 감소 비율을 지정하며 기본값은 0.9

6. Adam
Adam 옵티마이저 클래스
learning_rate 매개변수로 학습률을 지정하며 기본값은 0.001
모멘텀 최적화에 있는 그레이디언트 지수 감소 평균을 조절하기 위해 beta_1 매개변수가 있으며 기본값은 0.9
RMSprop에 있는 그레이디언트 제곱의 지수 감소 평균을 조절하기 위해 beta_2 매개변수가 있으며 기본값은 0.999


