# 합성곱(convolution)
# 마치 입력 데이터에 마법의 도장을 찍어서 유용한 특성만 드러나게 하는 것으로 비유할 수 있다
# 7장에서 사용한 밀집층에는 뉴런마다 입력 개수만큼의 가중치가 있다
# 즉 모든 입력에 가중치를 곱한다

# 인공 신경망은 처음에 가중치와 절편을 랜덤하게 초기화한 다음 에포크를 반복하면서 
# 경사 하강법 알고리즘을 사용하여 손실이 낮아지도록 최적의 가중치와 절편을 찾아간다
# 이것이 모델 훈련이다

# 예를 들어 밀집층에 뉴런이 3개 있다면 출력은 3개가 된다
# 입력 개수에 상관없이 동일하다
# 패션 MNIST 이미지에 잇는 784개의 픽셀을 입력받는 은닉층의 뉴런 개수가 100개면 
# 뉴런마다 하나씩 출력도 100개가 된다

# 합성곱은 밀집층의 계산과 조금 다르다
# 입력 데이터 전체에 가중치를 적용하는 것이 아니라 일부에 가중치를 곱한다
# 합성곱 층의 뉴런에 있는 가중치 개수는 하이퍼파라미터다

# 신경망 층의 뉴런을 그림으로 표현하면 서로 조밀하게 연결되어 있다
# 합성곱에서는 뉴런이 입력 위를 이동하면서 출력을 만들기 떄문에 이런 식으로 표현하기 어렵다
# 또 뉴런이라고 부르기도 어색하다

# 합성곱 신경망 (convolutional neural network, CNN)에서는 완전 연결 신경망과 달리
# 뉴런을 필터(filter)나 커널(kernel)이라고 부른다

# 완전 연결 신경망
# 완전 연결 층(밀집층)만 사용하여 만든 신경망을 완전 연결 신경망(밀집 신경망)이라고 부른다

# 합성곱의 장점은 1차원이 아니라 2차원 입력에도 적용할 수 있다는 것이다
# 입력이 2차원 배열이면 필터도 2차원이어야 한다
# 합성곱은 출력을 필터가 입력에 놓인 위치에 맞게 2차원으로 배치한다
# 합성곱 계산을 통해 얻은 출력을 특별히 특성 맵(feature map)이라고 부른다

# 뉴런 = 필터
# 가중치 = 커널
# 출력 = 특성 맵

# 밀집층에서 여러 개의 뉴런을 사용하듯이 합성곱 층에서도 여러 개의 필터를 사용한다

# 밀집층에 있는 뉴런의 가중치가 모두 다르듯이 합성곱 층에 있는 필터의 가중치(커널)도 모두 다르다
# 같은 가중치를 가진 필터를 여러 개 사용할 이유가 없다

# 실제 계산은 밀집층과 동일하게 단순히 입력과 가중치를 곱하는 것이지만 2차원 형태를 유지하는 점이 다르다
# 또 입력보다 훨씬 작은 크기의 커널을 사용하고 입력 위를 이동하면서 2차원 특성 맵을 만든다
# 이렇게 2차원 구조를 그대로 사용하기 때문에 합성곱 신경망이 이미지 처리 분야에서 뛰어난 성능을
# 발휘한다

# 케라스 합성곱 층
# 케라스의 층은 모두 keras.layers 패키지 아래 클래스로 구현되어 있다
# 특별히 입력 위를 (왼쪽에서 오른쪽으로, 위에서 아래로) 이동하는 합성곱은 Conv2D 클래스로 제공
from tensorflow import keras
keras.layers.Conv2D(10, kernel_size=(3, 3), activation='relu')

# Conv2D 클래스의 첫 번째 매개변수는 필터의 개수이다 
# kernel_szie 매개변수는 필터에 사용할 커널의 크기를 지정 
# 필터의 개수와 커널의 크기는 반드시 지정해야 하는 매개변수이다

# 커널의 크기는 하이퍼파라미터라서 여러 가지 값을 시도해 봐야 한다
# 하지만 보통 (3, 3)이나 (5, 5)크기가 권장된다

# 케라스 API를 사용해 합성곱 층을 만드려면 Dense 층을 사용했던 자리에 Conv2D층을 넣으면 된다
# 다만 kernel_size와 같이 추가적인 매개변수들을 고려해야 한다

# 일반적으로 1개 이상의 합성곱 층을 쓴 인공 신경망을 합성곱 신경망이라고 부른다
# 꼭 합성곱 층만 사용한 신경망을 합성곱 신경망이라고 부르는 것은 아니다
# 클래스에 대한 확률을 계산하려면  마지막 층에 클래스 개수만큼의 뉴런을 가진 밀집층을 두는 것이 일반적이다

# (4, 4) 크기의 입력에 (3, 3)크기의 커널을 적용하면 (2, 2)크기의 특성 맵이 나온다
# 커널 크기를 (3, 3)으로 두면서 출력의 크기를 입력과 동일하게 만드려면 
# 입력 배열의 주위를 가상의 원소로 채워 (6, 6)크기로 만들어주면 된다

# 입력 배열의 주위를 가상의 원소로 채우는 것을 패딩(padding)이라고 한다
# 실제 입력값이 아니기 때문에 패딩은 0으로 채운다
# 패딩의 역할은 순전히 커널이 도장을 찍을 횟수를 늘려주는 것밖에는 없다
# 실제 값은 0으로 채워져 있기 때문에 계산에 영향을 미치지는 않는다

# 입력과 특성 맵의 크기를 동일하게 만들기 위해 입력 주위에 0으로 패딩 하는 것을
# 세임 패딩(same padding)이라고 부른다
# 합성곱 신경망에서는 세임 패딩이 많이 사용된다
# 입력과 특성 맵의 크기를 동일하게 만드는 경우가 아주 많다

# 패딩 없이 순수한 입력 배열에서만 합성곱을 하여 특성 맵을 만드는 경우를 밸리드 패딩(valid padding)
# 이라고 한다 밸리드 패딩은 특성 맵의 크기가 줄어들 수밖에 없다

# 패딩을 하지 않는다면 모서리에 있는 값들은 커널 범위에 한 번만 들어간다
# 반면 다른 원소들은 2번 이상 커널과 계산된다
# 가운데 있는 원소들은 계산에 많이 포함되게 된다
# 만약 이 입력을 이미지라고 생각하면 모서리에 있는 중요한 정보가 특성 맵으로 잘 전달되지 않을 가능성이 높다
# 반면 가운데 있는 정보는 두드러지게 표현된다

# 적절한 패딩은 이미지의 주변에 있는 정보를 잃어버리지 않도록 도와준다
# 케라스 Conv2D 클래스에서는 padding 매개변수로 패딩을 지정할 수 있다
# 기본값은 valid로 밸리드 패딩을 나타낸다 
# 세임 패딩을 사용하려면 same으로 지정한다
keras.layers.Conv2D(10, kernel_size=(3, 3), activation='relu', padding='same')

# 합성곱 연산을 건너뛰는 칸도 지정할 수 있다
# 이런 이동의 크기를 스트라이드(stride)라고 한다
# 기본으로 스트라이드는 1이며 한 칸씩 이동한다
# 이 값은 Conv2D의 strides 매개변수의 기본값이다
keras.layers.Conv2D(10, kernel_szie=(3, 3), activation='relu', padding='same', strides=1)

# strides 매개변수는 오른쪽으로 이동하는 크기와 아래쪽으로 이동하는 크기를 (1, 1)과 같이
# 튜플을 사용해 각각 지정할 수 있다
# 하지만 커널의 이동 크기를 가로세로 방향으로 다르게 지정하는 경우는 거의 없다
# 1보다 큰 스트라이드를 사용하는 경우도 드물다
# 대부분 기본값을 그대로 사용하기 때문에 strides 매개변수는 잘 사용하지 않는다

