# 합성곱(convolution)
# 마치 입력 데이터에 마법의 도장을 찍어서 유용한 특성만 드러나게 하는 것으로 비유할 수 있다
# 7장에서 사용한 밀집층에는 뉴런마다 입력 개수만큼의 가중치가 있다
# 즉 모든 입력에 가중치를 곱한다

# 인공 신경망은 처음에 가중치와 절편을 랜덤하게 초기화한 다음 에포크를 반복하면서 
# 경사 하강법 알고리즘을 사용하여 손실이 낮아지도록 최적의 가중치와 절편을 찾아간다
# 이것이 모델 훈련이다

# 예를 들어 밀집층에 뉴런이 3개 있다면 출력은 3개가 된다
# 입력 개수에 상관없이 동일하다
# 패션 MNIST 이미지에 잇는 784개의 픽셀을 입력받는 은닉층의 뉴런 개수가 100개면 
# 뉴런마다 하나씩 출력도 100개가 된다

# 합성곱은 밀집층의 계산과 조금 다르다
# 입력 데이터 전체에 가중치를 적용하는 것이 아니라 일부에 가중치를 곱한다
# 합성곱 층의 뉴런에 있는 가중치 개수는 하이퍼파라미터다

# 신경망 층의 뉴런을 그림으로 표현하면 서로 조밀하게 연결되어 있다
# 합성곱에서는 뉴런이 입력 위를 이동하면서 출력을 만들기 떄문에 이런 식으로 표현하기 어렵다
# 또 뉴런이라고 부르기도 어색하다

# 합성곱 신경망 (convolutional neural network, CNN)에서는 완전 연결 신경망과 달리
# 뉴런을 필터(filter)나 커널(kernel)이라고 부른다

# 완전 연결 신경망
# 완전 연결 층(밀집층)만 사용하여 만든 신경망을 완전 연결 신경망(밀집 신경망)이라고 부른다

# 합성곱의 장점은 1차원이 아니라 2차원 입력에도 적용할 수 있다는 것이다
# 입력이 2차원 배열이면 필터도 2차원이어야 한다
# 합성곱은 출력을 필터가 입력에 놓인 위치에 맞게 2차원으로 배치한다
# 합성곱 계산을 통해 얻은 출력을 특별히 특성 맵(feature map)이라고 부른다

# 뉴런 = 필터
# 가중치 = 커널
# 출력 = 특성 맵

# 밀집층에서 여러 개의 뉴런을 사용하듯이 합성곱 층에서도 여러 개의 필터를 사용한다

# 밀집층에 있는 뉴런의 가중치가 모두 다르듯이 합성곱 층에 있는 필터의 가중치(커널)도 모두 다르다
# 같은 가중치를 가진 필터를 여러 개 사용할 이유가 없다

# 실제 계산은 밀집층과 동일하게 단순히 입력과 가중치를 곱하는 것이지만 2차원 형태를 유지하는 점이 다르다
# 또 입력보다 훨씬 작은 크기의 커널을 사용하고 입력 위를 이동하면서 2차원 특성 맵을 만든다
# 이렇게 2차원 구조를 그대로 사용하기 때문에 합성곱 신경망이 이미지 처리 분야에서 뛰어난 성능을
# 발휘한다

# 케라스 합성곱 층
# 케라스의 층은 모두 keras.layers 패키지 아래 클래스로 구현되어 있다
# 특별히 입력 위를 (왼쪽에서 오른쪽으로, 위에서 아래로) 이동하는 합성곱은 Conv2D 클래스로 제공
from tensorflow import keras
keras.layers.Conv2D(10, kernel_size=(3, 3), activation='relu')

# Conv2D 클래스의 첫 번째 매개변수는 필터의 개수이다 
# kernel_szie 매개변수는 필터에 사용할 커널의 크기를 지정 
# 필터의 개수와 커널의 크기는 반드시 지정해야 하는 매개변수이다

# 커널의 크기는 하이퍼파라미터라서 여러 가지 값을 시도해 봐야 한다
# 하지만 보통 (3, 3)이나 (5, 5)크기가 권장된다

# 케라스 API를 사용해 합성곱 층을 만드려면 Dense 층을 사용했던 자리에 Conv2D층을 넣으면 된다
# 다만 kernel_size와 같이 추가적인 매개변수들을 고려해야 한다

# 일반적으로 1개 이상의 합성곱 층을 쓴 인공 신경망을 합성곱 신경망이라고 부른다
# 꼭 합성곱 층만 사용한 신경망을 합성곱 신경망이라고 부르는 것은 아니다
# 클래스에 대한 확률을 계산하려면  마지막 층에 클래스 개수만큼의 뉴런을 가진 밀집층을 두는 것이 일반적이다

# (4, 4) 크기의 입력에 (3, 3)크기의 커널을 적용하면 (2, 2)크기의 특성 맵이 나온다
# 커널 크기를 (3, 3)으로 두면서 출력의 크기를 입력과 동일하게 만드려면 
# 입력 배열의 주위를 가상의 원소로 채워 (6, 6)크기로 만들어주면 된다

# 입력 배열의 주위를 가상의 원소로 채우는 것을 패딩(padding)이라고 한다
# 실제 입력값이 아니기 때문에 패딩은 0으로 채운다
# 패딩의 역할은 순전히 커널이 도장을 찍을 횟수를 늘려주는 것밖에는 없다
# 실제 값은 0으로 채워져 있기 때문에 계산에 영향을 미치지는 않는다

# 입력과 특성 맵의 크기를 동일하게 만들기 위해 입력 주위에 0으로 패딩 하는 것을
# 세임 패딩(same padding)이라고 부른다
# 합성곱 신경망에서는 세임 패딩이 많이 사용된다
# 입력과 특성 맵의 크기를 동일하게 만드는 경우가 아주 많다

# 패딩 없이 순수한 입력 배열에서만 합성곱을 하여 특성 맵을 만드는 경우를 밸리드 패딩(valid padding)
# 이라고 한다 밸리드 패딩은 특성 맵의 크기가 줄어들 수밖에 없다

# 패딩을 하지 않는다면 모서리에 있는 값들은 커널 범위에 한 번만 들어간다
# 반면 다른 원소들은 2번 이상 커널과 계산된다
# 가운데 있는 원소들은 계산에 많이 포함되게 된다
# 만약 이 입력을 이미지라고 생각하면 모서리에 있는 중요한 정보가 특성 맵으로 잘 전달되지 않을 가능성이 높다
# 반면 가운데 있는 정보는 두드러지게 표현된다

# 적절한 패딩은 이미지의 주변에 있는 정보를 잃어버리지 않도록 도와준다
# 케라스 Conv2D 클래스에서는 padding 매개변수로 패딩을 지정할 수 있다
# 기본값은 valid로 밸리드 패딩을 나타낸다 
# 세임 패딩을 사용하려면 same으로 지정한다
keras.layers.Conv2D(10, kernel_size=(3, 3), activation='relu', padding='same')

# 합성곱 연산을 건너뛰는 칸도 지정할 수 있다
# 이런 이동의 크기를 스트라이드(stride)라고 한다
# 기본으로 스트라이드는 1이며 한 칸씩 이동한다
# 이 값은 Conv2D의 strides 매개변수의 기본값이다
keras.layers.Conv2D(10, kernel_szie=(3, 3), activation='relu', padding='same', strides=1)

# strides 매개변수는 오른쪽으로 이동하는 크기와 아래쪽으로 이동하는 크기를 (1, 1)과 같이
# 튜플을 사용해 각각 지정할 수 있다
# 하지만 커널의 이동 크기를 가로세로 방향으로 다르게 지정하는 경우는 거의 없다
# 1보다 큰 스트라이드를 사용하는 경우도 드물다
# 대부분 기본값을 그대로 사용하기 때문에 strides 매개변수는 잘 사용하지 않는다

# 폴링(pooling)
# 합성곱 층에서 만든 특성 맵의 가로세로 크기를 줄이는 역할을 수행
# 하지만 특성맵의 개수는 줄이지 않는다
# 예를 들어 (2, 2, 3)크기의 특성 맵에 풀링을 적용하면 마지막 차원의 개수는 그대로 유지하고
# 너비와 높이만 줄어들어 (1, 1, 3)크기의 특성 맵이 된다

# 풀링도 합성곱처럼 입력 위를 지나가면서 도장을 찍는다
# 풀링에는 가중치가 없다
# 도장을 찍은 영역에서 가장 큰 값을 고르거나 평균값을 계산한다
# 이를 각각 최대 풀링(max pooling)과 평균 풀링(average pooling)이라고 부른다
# 특성 맵이 여러 개하면 동일한 작업을 반복한다 
# 합성곱 신경망에서는 합성곱 층과 풀링 층에서 출력되는 값을 모두 특성 맵이라고 부른다

# 풀링 영역은 커널과 다르게 겹치지 않는다
# 스트라이드는 풀링의 크기를 따라간다
# 풀링은 가중치가 없고 풀링 크기와 스트라이드가 같다 또 패딩도 없다

# 케라스에서 MaxPooling2D 클래스로 풀링을 수행할 수 있다
keras.layers.MaxPooling2D(2)

# MaxPooling2D의 첫 번째 매개변수로 풀링의 크기를 지정한다
# 대부분 풀링의 크기는 2로 가로세로 크기를 절반으로 줄인다
# 가로세로 방향의 풀링 크기를 다르게 하려면 첫 번째 매개변수를 정수의 튜플로 지정할 수 있다 (3, 3)
# 하지만 이런 경우는 극히 드물다

# 합성곱 층과 마찬가지로 strides와 padding 매개변수를 제공한다
# strides의 기본값은 자동으로 풀링의 크기이므로 따로 지정할 필요가 없다
# padding의 기본값은 valid로 패딩을 하지 않는다
# 풀링은 패딩을 하지 않기 떄문에 이 매개변수를 바꾸는 경우는 거의 없다

# 평균 풀링을 제공하는 클래스는 AveragePooling2D이다
# 평균을 계산하는 것만 빼면 MaxPooling2D와 동일하며 제공하는 매개변수도 같다
# 많은 경우 평균 풀링보다 최대 풀링을 많이 사용한다
# 평균 풀링은 특성 맵에 있는 중요한 정보를 (평균하여) 희석시킬 수 있기 떄문이다
# 풀링은 가로세로 방향으로만 진행한다

# 패딩은 텐서플로에서 자동으로 추가
# 합성곱 층은 활성화 함수로 렐루 함수를 많이 사용
# 풀링을 사용하는 이유는 합성곱에서 스트라이드를 크게 하여 특성 맵을 줄이는 것보다
# 풀링 층에서 크기를 줄이는 것이 경험적으로 더 나은 성능을 내기 떄문이다
# 합성곱 신경망은 이렇게 합성곱 층에서 특성 맵을 생성하고 풀링에서 크기를 줄이는 구조가 쌍을 이룬다

# 흑백 이미지는 2차원 배열로 표현할 수 있다
# 컬러 이미지는 RGB(빨강, 초록, 파랑) 채널로 구성되어 있기 때문에 컴퓨터는 이를 3차원 배열로 표시한다
# 3차원 배열의 커널 배열의 깊이는 항상 입력의 깊이와 같다 
# ex) 입력 (4, 4, 3) -> 커널 (3, 3, 3)



