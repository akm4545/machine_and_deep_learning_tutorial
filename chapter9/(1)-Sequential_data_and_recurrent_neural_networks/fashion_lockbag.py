# 순차 데이터(sequential data)
# 텍스트나 시계열 데이터(time series data)와 같이 순서에
# 의미가 있는 데이터를 말한다
# 예를 들어 I am a boy는 가능하지만 boy am a I는 불가능하다

# 시계열 데이터는 순서를 유지하며 신경망에 주입해야 한다
# 단어의 순서를 마구 섞어서 주입하면 안 된다
# 따라서 순차 데이터를 다룰 때는 이전에 입력한 데이터를 기억하는 기능이 필요하다
# 예를 들어 별로지만 추천해요 에서 추천해요가 입력될 때 별로지만을 기억하고 있어야
# 이 댓글을 무조건 긍정적이라고 판단하지 않을 것이다

# 완전 연결 신경망이나 합성곱 신경망은 이런 기억 장치가 없다
# 하나의 샘플(또는 하나의 배치)을 사용하여 정방향 계산을 수행하고 나면 그 샘플은
# 버려지고 다음 샘플을 처리할 때 재사용하지 않는다

# 피드포워드 신경망(feedforward neural network, FFNN)
# 입력 데이터의 흐름이 앞으로만 전달되는 신경망 - 완전 연결 신경망, 합성곱 신경망

# 다음 샘플을 위해서 이전 데이터가 신경망 층에 순환되는 신경망이 순환 신경망이다

# 순환 신경망(recurrent neural network, RNN)
# 일반적인 완전 연결 신경망과 거의 비슷하다
# 완전 연결 신경망에 이전 데이터의 처리 흐름을 순환하는 고리 하나만 추가하면 된다
# 뉴런의 출력이 다시 자기 자신으로 전달된다
# 즉 어떤 샘플을 처리할 때 바로 이전에 사용했던 데이터를 재사용하는 셈이다

# 순환 신겸망에서는 이전 샘플에 대한 기억을 가지고 있다고 종종 말한다
# 샘플을 처리하는 한 단계를 타임스텝(timestep)이라고 말한다

# 순환 신경망은 이전 타임스텝의 샘플을 기억하지만 타임스텝이 오래될수록 
# 순환되는 정보는 희미해진다

# 순환 신경망에서는 특별히 층을 셀(cell)이라고 부른다
# 한 셀에는 여러 개의 뉴런이 있지만 완전 연결 신경망과는 달리 뉴런을 모두 표시하지 않고
# 하나의 셀로 층을 표현한다 또 셀의 출력을 은닉 상태(hidden state)라고 부른다

# 합성곱 신경망에서처럼 신경망의 구조마다 조금씩 부르는 이름이 다를 수 있다
# 하지만 기본 구조는 같다 
# 입력에 어떤 가중치를 곱하고 활성화 함수를 통과시켜 다음 층으로 보낸다
# 달라지는 것은 층의 출력(즉 은닉 상태)을 다음 타임 스텝에 재사용한다는 것뿐이다

# 일반적으로 은닉층의 활성화 함수로는 하이퍼볼릭 탄젠트(hyperbolic tangent) 함수인 tanh가 많이
# 사용된다
# tanh 함수로 s자 모양을 띠기 때문에 종종 시그모이드 함수라고 부르기도 한다
# tanh 함수는 시그모이드 함수와는 달리 -1~1 사이의 범위를 가진다

# 다른 신경망과 마찬가지로 순환 신경망 그림에도 번거로움을 피하기 위해 활성화 함수를 표시하지
# 않는 경우가 많다 

# 합성곱 신경망과 같은 피드포워드 신경망에서 뉴런은 입력과 가중치를 곱한다
# 순환 신경망에서도 동일하다 
# 다만 순환 신경망의 뉴런은 가중치가 하나 더 있는데 이전 타임스텝의 은닉 상태에
# 곱해지는 가중치다
# 셀은 입력과 이전 타임스텝의 은닉 상태를 사용하여 현재 타임스텝의 은닉 상태를 

# 순환 신경망을 타임스텝마다 그릴 수 있는데 이런 그림을 보고 셀을 타임스텝으로 펼쳤다고 말한다

# 순환층에 입력되는 특성의 개수가 4개이고 순환층의 뉴런이 3개인 순환 신경망의 셀에서 필요한 가중치 크기를 계산

# 입력층과 순환층의 뉴런이 모두 완전 연결되기 떄문에 가중치 Wx의 크기는 4 * 3 = 12개가 된다

# 이전 타임스텝의 은닉 상태는 다음 타임스텝의 뉴런에 완전히 연결된다
# 따라서 이 순환층에서 은닉 상태를 위한 가중치 Wh는 3 * 3 = 9개이다

# 모델 파아미터 개수 계산
# 가중치에 절편을 더한다
# 여기엔 각 뉴런마다 하나의 절편이 있다 
# 따라서 이 순환층은 모두 12 + 9 + 3 = 24개의 모델 파라미터를 가지고 있다
# 은닉 상태가 모든 뉴런에 순환되기 때문에 완전 연결 신경망처럼 그림으로 표현하지는 너무 어렵다

# 모델 파라미터 수 = Wx + Wh + 절편 = 12 + 9 + 3 = 24

# 순환층은 일반적으로 샘플라마다 2개의 차원을 가진다
# 보통 하나의 샘플을 하나의 시퀀스(sequence)라고 말한다
# 시퀀스 안에는 여러 개의 아이템이 들어있다 
# 여기에서 시퀀스의 길이가 타임스텝의 길이가 된다

# 예를 들어 어떤 샘플에 I am a boy란 문장이 들어 있다고 가정했을 시 
# 이 샘플은 4개의 단어로 이루어져 있다 
# 각 단어를 3개의 어떤 숫자로 표현한다고 가정 (1, 4, 3)

# 이런 입력이 순환층을 통과하면 두 번째, 세 번째 차원이 사라지고 순환층의 뉴런 개수만큼 출력된다
# 하나의 샘플은 시퀀스 길이(여기에서는 단어 개수)와 단어 표현의 2차원 배열이다
# 순환층을 통과하면 1차원 배열로 바뀐다
# 1차원 배열의 크기는 순환층의 뉴런 개수에 의해 결정된다

# 순환층은 기본적으로 마지막 타임스텝의 은닉 상태만 출력으로 내보낸다
# 이는 마치 입력된 시퀀스 길이를 모두 읽어서 정보를 마지막에 은닉 상태에 압축하여 
# 전달하는 것처럼 볼 수 있다
# 이런 특징으로 인해 순환 신경망이 정보를 기억하는 메모리를 가진다고 표현하고 
# 순환 신겸앙이 순차 데이터에 잘 맞는 이유이다

# 순환 신경망도 완전 연결 신경망이나 합성곱 신경망처럼 여러 개의 층을 쌓을 수 있다
# 셀의 입력은 샘플마다 타임스텝과 단어 표현으로 이루어진 2차원 배열이어야 한다
# 따라서 첫 번째 셀이 마지막 타임스텝의 은닉 상태만 출력해서는 안 된다
# 이런 경우에는 마지막 셀을 제외한 다른 모든 셀은 모든 타임스텝의 은닉 상태를 출력한다

# 첫 번째 셀은 모든 타임스텝의 은닉 상태를 출력하고 두 번째 셀은 마지막 타임스텝의
# 은닉 상태만 출력한다

# 합성곱 신경망과 마찬가지로 순환 신경망도 마지막에는 밀집층을 두어 클래스를 분류한다
# 다중 분류일 경우에는 출력층에 클래스 개수만큼 뉴런을 두고 소프트맥스 활성화 함수를 사용
# 이진 분류일 경우에는 하나의 뉴런을 두고 시그모이드 활성화 함수를 사용

# 합성곱 신경망과 다른 점은 마지막 셀의 출력이 1차원이기 떄문에 Flatten 클래스로 펼칠
# 필요가 없다
# 셀픠 출력을 그대로 밀집층에 사용할 수 있다









