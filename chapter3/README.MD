지도 학습 알고리즘은 크게 분류와 회귀로 나뉜다
도미, 빙어를 분류한 알고리즘은 분류에 속한다
회귀는 클래스 중 하나로 분류하는 것이 아니라 임의의 어떤 숫자를 예측하는 문제
ex)내년도 경제 성장률 예측, 배달 도착시간 예측

회귀
회귀는 19세기 통계학자이자 사회학자인 프랜시스 골랜턴이 처음 사용
키가 큰 사람의 아이가 부모보다 더 크지 않는다는 사실을 관찰하고 이를 평균으로 회귀한다고 표현
두 변수 사이의 상관관계를 분석하는 방법을 회귀라고 부름
임의의 수치를 예측하는 문제 따라서 타깃값도 임의의 수치가 된다

k-최근접 이웃
k-최근접 이웃 분류 알고리즘
예측 샘플에 가장 가까운 샘플 k개를 선택한다 -> 이 샘플들의 클래스를 확인하여 다수 클래스를 새로운 샘플의 클래스로 예측한다

k-최근접 이웃 회귀 
예측 샘플에 가장 가까운 샘플 k개를 선택한다 -> 회귀이기 때문에 이웃한 샘플의 타깃은 어떤 클래스가 아니라 임의의 수치이다 -> 이웃 샘플의 평균을 구해 예측 샘플의 타깃값을 예측한다
ex) 이웃 샘플의 타깃값이 각각 100, 80, 60이라고 한다면 샘플 x의 예측 타깃값은 80
k-최근접 이웃 알고리즘을 사용해 회귀 문제를 푼다 가장 가까운 이웃 샘플을 찾고 이 샘플들의 타깃값을 평균하여 예측으로 삼는다

결정계수(R²)
대표적인 회귀 문제의 성능 측정 도구 
1에 가까울수록 좋고 0에 가깝다면 성능이 나쁜 모델이다

과대적합
모델의 훈련 세트 성능이 테스트 세트 성능보다 훨씬 높을 때 일어난다
모델이 훈련 세트에 너무 집착해서 데이터에 내재된 거시적인 패턴을 감지하지 못한다

과소적합
훈련 세트와 테스트 세트 성능이 모두 동일하게 낮거나 테스트 세트 성능이 오히려 더 높을 때 일어난다
이런 경우 더 복잡한 모델을 사용해 훈련 세트에 잘 맞는 모델을 만들어야 한다

scikit-learn
KNeighborsRegressor
k-최근접 이웃 회귀 모델을 만드는 사이킷런 클래스
n_neighbors 매개변수로 이웃의 개수를 지정한다 기본값은 5
다른 매개변수는 KNeighborsClassifier 클래스와 거의 동일하다

mean_absolute_error()
회귀 모델의 평균 절댓값 오차를 계산
첫 번째 매개변수는 타깃, 두 번째 매개변수는 예측값을 전달 
이와 비슷한 함수로는 평균 제곱 오차를 계산하는 mean_squared_error()가 있다
이 함수는 타깃과 에측을 뺀 값을 제곱한 다음 전체 샘플에 대해 평균한 값을 반환한다

numpy
reshape()
배열의 크기를 바꾸는 메서드 
바꾸고자 하는 배열의 크기를 매개변수로 전달한다
바꾸기 전후의 배열 원소 개수는 동일해야 한다
넘파이는 종종 배열의 메서드와 동일한 함수를 별도로 제공한다 이 때 함수의 첫 번재 매개변수는 바꾸고자 하는 배열이다
ex) test_array.reshape(2, 2) -> np.reshape(test_array, (2, 2)) 