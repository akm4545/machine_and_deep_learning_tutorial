# 점진적 학습
# 온라인 학습이라고도 부른다
# 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 조금씩 더 훈련하는 방식
# 훈련에 사용한 데이터를 모두 유지할 필요도 없고 앞서 학습한 내용이 사라지지도 않는다

# 확률적 경사 하강법(Stochastic gradient descent)
# 대표적인 점진적 학습 알고리즘 
# 확률적 경사 하강법에서 확률적이란 말은 무작위하게 혹은 랜덤하게의 기술적인 표현
# 경사 하강법은 경사를 따라 내려가는 방법을 말한다

# 경사 하강법
# 가장 가파른 경사를 따라 원하는 지점에 도달하는 것이 목표
# 가장 가파른 길을 찾아 내려오지만 조금씩 내려오는 것이 중요하다
# 이렇게 내려오는 과정이 경사 하강법 모델을 훈련하는 것이다

# 확률적
# 경사 하강법은 훈련 세트를 사용하여 가장 가파른 길을 찾는다
# 전체 샘플을 사용하지 않고 딱 하나의 샘플을 훈련 세트에서 랜덤하게 골라 가장 가파른 길을 찾는다

# 확률적 경사 하강법은 훈련 세트에서 랜덤하게 하나의 샘플을 선택하여 가파른 경사를 조금 내려간다
# 그다음 훈련 세트에서 랜덤하게 또 다른 샘플을 하나 선택하여 경사를 조금 내려간다
# 이런 식으로 전체 샘플을 모두 사용할 때까지 계속한다
# 모든 샘플을 다 사용해도 경사의 끝에 도달하지 못했으면 다시 처음부터 시작한다

# 에포크(epoch)
# 확률적 경사 하강법에서 훈련 세트를 한 번 모두 사용하는 과정을 에포크라고 부른다
# 일반적으로 경사 하강법은 수십, 수백 번 이상 에포크를 수행한다

# 미니배치 경사 하강법(minibatch gradient descent)
# 1개의 샘플이 아니라 무작위로 몇 개의 샘플을 선택해서 경사를 내려가는 방식
# 실전에서 아주 많이 사용한다

# 배치 경사 하강법(batch gradient descent)
# 극단적으로 한 번 경사로를 따라 이동하기 위해 전체 샘플을 사용하는 방식
# 전체 데이터를 사용하기 때문에 가장 안정적인 방법이지만 그만큼 컴퓨터 자원을 많이 사용한다
# 어떤 경우는 데이터가 너무 많아 한 번에 전체 데이털르 모두 읽을 수 없을수도 있다

# 이 때문에 훈련 데이터가 모두 준비되어 있지 않고 매일매일 업데이트 되어도 학습을 계속 이어나갈 수 있다

# 신경망 알고리즘은 학률적 경사 하강법을 반드시 사용하는 알고리즘
# 신경망은 일반적으로 많은 데이터를 사용하기 때문에 한 번에 모든 데이터를 사용하기 어렵다
# 또 모델이 매우 복잡하기 때문에 수학적인 방법으로 해답을 얻기어렵다
# 신경망 모델은 확률적 경사 하강법이나 미니배치 경사 하강법을 사용한다

# 손실 함수(loss function)
# 어떤 문제에서 머신러닝 알고리즘이 얼마나 엉터리인지를 측정하는 기준
# 손실 함수의 값이 작을수록 좋다
# 하지만 어떤 값이 최솟값인지는 알지 못한다
# 가능한 많이 찾아보고 만족할만한 수준이라면 다 내려왔다고 인정해야 한다

# 비용 함수(cost function)는 손실 함수의 다른 말이다
# 엄밀히 말하면 손실 함수는 샘플 하나에 대한 손실을 정의하고 비용 함수는 훈련 세트에 있는 모든 샘플에 대한 손실 함수의 합을 말한다
# 보통 이 둘을 엄격히 구분하지 않고 섞어서 사용한다


