# K-평균(k-means) 군집 알고리즘
# 비지도 학습에서 평균값을 자동으로 찾아준다

# 이 평균값이 클러스터의 중심에 위치하기 때문에
# 클러스터 중심(cluster center) 또는 센트로이드(centroid)라고 부른다

# k-평균 알고리즘의 작동 방식
# 1. 무작위로 k개의 클러스터 중심을 정한다
# 2. 각 샘플에서 가장 가까운 클러스터 중심을 찾아 해당 클러스터의 샘플로 지정
# 3. 클러스터에 속한 샘플의 평균값으로 클러스터 중심을 변경
# 4. 클러스터 중심에 변화가 없을 때까지 2번으로 돌아가 반복

# wget으로 데이터 다운로드
!wget https://bit.ly/fruits_300_data -O fruits_300.npy

# 넘파이 np.load() 함수를 사용해 npy 파일을 읽어 넘파이 배열을 준비
# k-평균모델을 훈련하기 위해 (샘플 개수, 너비, 높이) 크기의 3차원 배열을 (샘플 개수, 너비 X 높이)크기를 가진 2차원 배열로 변경
import numpy as np

fruits = np.load('fruits_300.npy')
fruits_2d = fruits.reshape(-1, 100 * 100)

# 사이킷런의 k-평균 알고리즘은 sklean.cluster 모듈 아래 KMeans 클래스에 구현되어 있다
# 이 클래스에서 설정할 매개변수는 클러스터 개수를 지정하는 n_clusters이다 
# 여기서는 클러스터 개수를 3으로 지정한다

# 사용법도 다른 클래스와 비슷하다
# 다만 비지도 학습이므로 fit() 메서드에서 타깃 데이터를 사용하지 않는다
from sklearn.cluster import KMeans

km = KMeans(n_clusters=3, random_state=42)
km.fit(fruits_2d)

# 군집된 결과는 KMeans 클래스 객체의 labels_ 속성에 저장된다
# labels_ 배열의 길이는 샘플 개수와 같다 
# 이 배열은 각 샘플이 어떤 레이블에 해당되는지 나타낸다

# n_cluster=3으로 지정했기 때문에 labels_ 배열의 값은 0, 1, 2 중 하나이다
print(km.labels_)
# [2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
#  2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 2 0 2
#  2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0
#  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
#  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
#  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
#  1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
#  1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
#  1 1 1 1]

# 레이블값 0, 1, 2와 레이블 순서에는 어떤 의미도 없다
# 실제 레이블 0, 1, 2가 어떤 과일 사진을 주로 모았는지 알아보려면 직접 이미지를 출력하는 것이 최선이다

# 레이블 0, 1, 2로 모은 샘플의 개수 확인
print(np.unique(km.labels_, return_counts=True))
# (array([0, 1, 2], dtype=int32), array([111,  98,  91]))

# 첫 번째 클러스터(레이블 0)가 111개의 샘플 수집
# 두 번째 클러스터(레이블 1)가 98개의 샘플 수집
# 세 번째 클러스터(레이블 2)가 91개의 샘플 수집

# 각 클러스터가 어떤 이미지를 나타냈는지 그림으로 출력하기 위해 간단한 유틸리티 함수 draw_fruits() 만들기
import matplotlib.pyplot as plt

# 3차원 배열을 입력받아 가로로 10개씩 이미지 출력
# 샘플 개수에 따라 행과 열의 개수를 계산하고 figsize를 지정
# figsize는 ratio 매개변수에 비례하여 커진다 
# ratio의 기본값은 1

# 2중 for 반복문을 사용하여 먼저 첫 번째 행을 따라 이미지를 그린다
# 두 번째 행의 이미지를 그리는 식으로 계속된다
def draw_fruits(arr, ratio=1):
    # n은 샘플 개수
    n = len(arr) 
    # 한 줄에 10개씩 이미지를 그린다
    # 샘플 개수를 10으로 나누어 전체 행 개수를 계산한다
    rows = int(np.ceil(n / 10))
    # 행이 1개이면 열의 개수는 샘플 개수이다 
    # 그렇지 않으면 10개이다
    cols = n if rows < 2 else 10

    fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze=False)

    for i in range(rows):
        for j in range(cols):
            # n 개까지만 그린다
            if i * 10 + j < n: 
                axs[i, j].imshow(arr[i * 10 + j], cmap='gray_r')
            axs[i, j].axis('off')
    
    plt.show()

# 이 함수를 사용해 레이블이 0인 과일 사진을 모두 그린다
# km.labels_==0과 같이 쓰면 km.labels_ 배열에서 값이 0인 위치는 True, 그 외는 모두 False가 된다
# 넘파이의 불리언 인덱싱 사용

draw_fruits(fruits[km.labels_==0])
# 해당 클러스터는 대부분 파인애플이다

# 다른 두 클러스터도 출력
draw_fruits(fruits[km.labels_==1])
draw_fruits(fruits[km.labels_==2])

# 레이블이 1인 클러스터는 바나나로만 이루어져 있고 
# 레이블이 2인 클러스터는 사과로만 이루어져 있다
# 하지만 레이블이 0인 클러스터는 파인애플에 사과 9개와 바나나 2개가 섞여 있다
# k-평균 알고리즘이 이 샘플들을 완벽하게 구별해내지는 못했지만 훈련 데이터에 타깃 레이블을 전혀 제공하지 않았음에도
# 스스로 비슷한 샘플들을 아주 잘 모았다

# KMeans 클래스가 최종적으로 찾은 클러스터 중심은 cluster_centers_ 송성에 저장되어 있다
# 이 배열은 fruits_2d 샘플의 클러스터 중심이기 떄문에 각 중심을 이미지로 출력하려면 100 X 100 크기의 2차원 배열로바꿔야 한다

draw_fruits(km.cluster_centers_.reshape(-1, 100, 100), ratio=3)

# KMeans 클래스는 훈련 데이터 샘플에서 클러스터 중심까지 거리로 변환해 주는 transform() 메서드를 가지고 있다
# transform() 메서드가 있다는 것은 마치 StandardScaler 클래스처럼 특성값을 변환하는 도구로 사용할 수 있다는 의미이다

# 인덱스가 100인 샘플에 transform() 메서드를 적용
# fit() 메서드와 마찬가지로 2차원 배열이 필요하다
# fruits_2d[100] 처럼 쓰면 (10000,) 크기의 배열이 되므로 에러가 발생한다
# 슬라이싱 연산자를 사용해서 (1, 10000) 크기의 배열을 전달
print(km.transform(fruits_2d[100:101]))
# [[3393.8136117  8837.37750892 5267.70439881]]

# 하나의 샘플을 전달했기 때문에 반환된 배열은 크기가 (1, 클러스터 개수)인 2차원 배열이다
# 첫 번째 클러스터(레이블 0), 두 번째 클러스터(레이블 1)가 각각 첫 번째 원소, 두 번째 원소의 값이다
# 첫 번째 클러스터까지의 거리가 3393.8로 가장 작다
# 이 샘플은 레이블 0에 속한다

# KMeans 클래스는 가장 가까운 클러스터 중심을 예측 클래스로 출력하는 predict() 메서드를 제공한다
print(km.predict(fruits_2d[100:101]))
# [0]

# transform()의 결과에서 짐작 가능하듯이 레이블 0으로 예측한다

# 클러스터 중심을 그려보았을 때 레이블 0은 파인애플이었으므로 이 샘플은 파인애플일 것이다
draw_fruits(fruits[100:101])

# k-평균 알고리즘은 반복적으로 클러스터 중심을 옮기면서 최적의 클러스터를 찾는다
# 알고리즘이 반복한 횟수는 KMeans 클래스의 n_iter_속성에 저장
print(km.n_iter_)
# 4

# 클러스터 중심을 특성 공학처럼 사용해 데이터셋을 저차원(이 경우에는 10000에서 3으로 줄인다)으로 변환할 수 있다
# 또는 가장 가까운 거리에 있는 크러스터 중심을 샘플의 예측값으로 사용할 수 있다

# 이번 예제는 타깃값을 사용하지 않았지만 약간의 편법을 사용했다
# n_clusters를 3으로 지정한 것은 타깃에 대한 정보를 활용한 셈이다
# 실전에서는 클러스터 개수조차 알 수 없다


# k-평균 알고리즘의 단점 중 하나는 클러스터 개수를 사전에 지정해야 한다는 것이다
# 군집 알고리즘에서 적절한 k 값을 찾기 위한 완벽한 방법은 없다
# 몇 가지 도구가 있지만 저마다 장단점이 있다

# 이너셔(inertia)
# k-평군 알고리즘은 클러스터 중심과 클러스터에 속한 샘플 사이의 거리를 잴 수 있다
# 이 거리의 제곱 합을 이너셔라고 부른다
# 이너셔는 클러스터에 속한 샘플이 얼마나 가깝게 모여 있는지를 낱내는 값으로 생각할 수 있다
# 일반적으로 클러스터 개수가 늘어나면 클러스터 개개의 크기는 줄어들기 때문에 이너셔도 줄어든다

# 엘보우(elbow)
# 적절한 클러스터 개수를 찾기 위한 대표적인 방법
# 엘보우 방법은 클러스터 개수를 늘려가면서 이너셔의 변화를 관찰하여 최적의 클러스터 개수를 찾는 방법

# 클러스터 개수를 증가시키면서 이너셔를 그래프로 그리면 감소하는 속도가 꺾이는 지점이 있다
# 이 지점부터는 클러스터 개수를 늘려도 클러스터에 잘 밀집된 정도가 크게 개선되지 않는다
# 즉 이니셔가 크게 줄어들지 않는다 이 지점이 마치 팔꿈치 모양이어서 엘보우 방법이라 부른다

# 과일 데이터셋을 사용해 이너셔 계산
# KMeans 클래스는 자동으로 이너셔를 계산해서 inertia_속성으로 제공 

# 클러스터 개수 k를 2~6까지 바꿔가며 KMeans 클래스를 5번 훈련
# fit() 메서드로 모델을 훈련한 후 inertia_ 속성에 저장된 이니셔 값을 리스트에 추가
# 마지막으로 출력
inertia = []

for k in range(2, 7):
    km = KMeans(n_clusters=k, n_init='auto', random_state=42)
    km.fit(fruits_2d)
    inertia.append(km.inertia_)

plt.plot(range(2, 7), inertia)
plt.xlabel('k')
plt.ylabel('inertia')
plt.show()

# 이 그래프는 꺾이는 지점이 두드러지지는 않지만 k = 3에서 그래프의 기울기가 조금 바뀐다
# 엘보우 지점보다 클러스터 개수가 많아지면 이너셔의 변화가 줄어들면서 군집 효과도 줄어든다